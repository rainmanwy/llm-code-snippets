{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.docstore.document import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Create a list of city documents.\n",
    "2. Initialize the embedding model.\n",
    "3. Create the vector database.\n",
    "4. Query the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: create documents\n",
    "docs = []\n",
    "for city, country, city_id in [('北京', 'China', '101010100'),\n",
    "                                ('上海', 'China', '101020100'),\n",
    "                                ('成都', 'China', '101270101'),\n",
    "                                ('Washington', 'US', 'ws'),\n",
    "                                ('Tokyo', 'Japan', 'tk')]:\n",
    "    docs.append(Document(page_content=city, metadata={'country': country, 'city_id': city_id}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name D:/workspace/model/bge-reranker-v2-m3. Creating a new one with mean pooling.\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at D:/workspace/model/bge-reranker-v2-m3 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Initialize embedding model\n",
    "# Remember to change the location of the model to your local path.\n",
    "embeddings = HuggingFaceEmbeddings(model_name='D:/workspace/model/bge-reranker-v2-m3',\n",
    "                                    model_kwargs={'device': 'cpu'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create vector database\n",
    "vector_db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "r = embeddings.embed_documents(['成都'])\n",
    "print(len(r[0]))\n",
    "\n",
    "sentences = [\n",
    "    '日本的首都是哪里',\n",
    "    '美国的首都是哪个城市？',\n",
    "    '\"窗含西岭千秋雪，门泊东吴万里船\"是描写哪个城市？',\n",
    "    '”晓看红湿处，花重锦官城“描写哪个城市？',\n",
    "    '明朝的京城',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "                                             Query       Similarity Research\n",
      "************************************************************\n",
      "                                  日本的首都是哪里               Washington\n",
      "                            美国的首都是哪个城市？               Washington\n",
      "  \"窗含西岭千秋雪，门泊东吴万里船\"是描写哪个城市？               Washington\n",
      "          ”晓看红湿处，花重锦官城“描写哪个城市？               Washington\n",
      "                                        明朝的京城                     上海\n"
     ]
    }
   ],
   "source": [
    "def alignment(str1, space, align = 'left'):\n",
    "    length = len(str1.encode('gb2312'))\n",
    "    space = space - length if space >=length else 0\n",
    "    if align == 'left':\n",
    "        str1 = str1 + ' ' * space\n",
    "    elif align == 'right':\n",
    "        str1 = ' '* space +str1\n",
    "    elif align == 'center':\n",
    "        str1 = ' ' * (space //2) +str1 + ' '* (space - space // 2)\n",
    "    return str1\n",
    "\n",
    "\n",
    "print('*' * 60)\n",
    "print('%50s %25s' % ('Query', 'Similarity Research'))\n",
    "print('*' * 60)\n",
    "for sentence in sentences:\n",
    "    results = vector_db.similarity_search_with_score(sentence, k=1)\n",
    "    result = results[0][0].page_content\n",
    "    print(alignment(sentence, 50, 'right'), alignment(result, 25, 'right'), sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过输出的字符串，基于相关性，查找最接近的城市"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import FlagReranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to change the location of the model to your local path.\n",
    "RERANKER = FlagReranker(\n",
    "            model_name_or_path='D:/workspace/model/bge-reranker-v2-m3',\n",
    "            devices=['cpu'],\n",
    "            trust_remote_code=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "                                             Query       Similarity Research\n",
      "************************************************************\n",
      "                                  日本的首都是哪里                    Tokyo\n",
      "                            美国的首都是哪个城市？               Washington\n",
      "  \"窗含西岭千秋雪，门泊东吴万里船\"是描写哪个城市？                     成都\n",
      "          ”晓看红湿处，花重锦官城“描写哪个城市？                     成都\n",
      "                                        明朝的京城                     北京\n"
     ]
    }
   ],
   "source": [
    "print('*' * 60)\n",
    "print('%50s %25s' % ('Query', 'Similarity Research'))\n",
    "print('*' * 60)\n",
    "for sentence in sentences:\n",
    "    results = vector_db.similarity_search_with_score(sentence, k=5)\n",
    "\n",
    "    pair_list = []\n",
    "    for result in results:\n",
    "        pair_list.append((sentence, result[0].page_content))\n",
    "    scores = RERANKER.compute_score(pair_list, normalize=True)\n",
    "    sorted_index = sorted(range(len(scores)), key=lambda k: scores[k], reverse=True)\n",
    "\n",
    "    result = results[sorted_index[0]][0].page_content\n",
    "    print(alignment(sentence, 50, 'right'), alignment(result, 25, 'right'), sep='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
